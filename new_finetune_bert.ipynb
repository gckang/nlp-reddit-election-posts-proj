{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline, Trainer, TrainingArguments\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original classifier: Linear(in_features=768, out_features=5, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original classifier: {model.classifier}\")  # Should show a layer with 5 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.num_labels = 3\n",
    "model.classifier = nn.Linear(model.config.hidden_size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated classifier: Linear(in_features=768, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Updated classifier: {model.classifier}\")  # Should now show 3 outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel(file):\n",
    "    df = pd.read_csv(file)\n",
    "    print(df.head(5))\n",
    "\n",
    "    df = df.rename(columns={'sentiment': 'label'})\n",
    "\n",
    "    def map_labels(rating):\n",
    "        if rating == \"negative\":\n",
    "            return 0  # Negative\n",
    "        elif rating == \"neutral\":\n",
    "            return 1  # Neutral\n",
    "        else:\n",
    "            return 2\n",
    "        \n",
    "    df['label'] = df['label'].apply(map_labels)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  created_utc  ups  \\\n",
      "0  Very true, but the problem is that even at tha...   1590975768    1   \n",
      "1  Youâ€™re full of crap.\\r\\n\\r\\nYou present no fac...   1590984758    2   \n",
      "2  Politically speaking, there is absolutely no w...   1590995791   29   \n",
      "3  This is what happens when the president of the...   1591008451    1   \n",
      "4  Silly comment. No matter how bad a president h...   1591013669    1   \n",
      "\n",
      "   subreddit    neg    neu    pos  compound sentiment  \n",
      "0  democrats  0.239  0.645  0.116   -0.9769  negative  \n",
      "1  democrats  0.163  0.692  0.145   -0.1101  negative  \n",
      "2  democrats  0.062  0.823  0.115    0.9925  positive  \n",
      "3  democrats  0.000  0.903  0.097    0.4215  positive  \n",
      "4  democrats  0.261  0.647  0.092   -0.8060  negative  \n",
      "                                                text  created_utc  ups  \\\n",
      "0  They sure are making the rounds today. Our sys...   1717200019    1   \n",
      "1  I would say itâ€™s both. Because after a super c...   1717200509    6   \n",
      "2  And in the same breath they claim the convicti...   1717204445   61   \n",
      "3  According to the writer, if it were a movement...   1717209701    3   \n",
      "4  I'll let Charles Pierce says my opinion:\\r\\n\\r...   1717216835  123   \n",
      "\n",
      "   subreddit    neg    neu    pos  compound sentiment  \n",
      "0  democrats  0.104  0.782  0.113    0.2005  positive  \n",
      "1  democrats  0.000  0.865  0.135    0.5994  positive  \n",
      "2  democrats  0.000  0.843  0.157    0.7783  positive  \n",
      "3  democrats  0.079  0.870  0.052   -0.2617  negative  \n",
      "4  democrats  0.079  0.806  0.115    0.6240  positive  \n"
     ]
    }
   ],
   "source": [
    "df2020 = relabel('./data/labeled_comments_2020.csv')\n",
    "df2024 = relabel('./data/labeled_comments_2024.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Very true, but the problem is that even at tha...</td>\n",
       "      <td>1590975768</td>\n",
       "      <td>1</td>\n",
       "      <td>democrats</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.9769</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Youâ€™re full of crap.\\r\\n\\r\\nYou present no fac...</td>\n",
       "      <td>1590984758</td>\n",
       "      <td>2</td>\n",
       "      <td>democrats</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-0.1101</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Politically speaking, there is absolutely no w...</td>\n",
       "      <td>1590995791</td>\n",
       "      <td>29</td>\n",
       "      <td>democrats</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is what happens when the president of the...</td>\n",
       "      <td>1591008451</td>\n",
       "      <td>1</td>\n",
       "      <td>democrats</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Silly comment. No matter how bad a president h...</td>\n",
       "      <td>1591013669</td>\n",
       "      <td>1</td>\n",
       "      <td>democrats</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.8060</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  created_utc  ups  \\\n",
       "0  Very true, but the problem is that even at tha...   1590975768    1   \n",
       "1  Youâ€™re full of crap.\\r\\n\\r\\nYou present no fac...   1590984758    2   \n",
       "2  Politically speaking, there is absolutely no w...   1590995791   29   \n",
       "3  This is what happens when the president of the...   1591008451    1   \n",
       "4  Silly comment. No matter how bad a president h...   1591013669    1   \n",
       "\n",
       "   subreddit    neg    neu    pos  compound  label  \n",
       "0  democrats  0.239  0.645  0.116   -0.9769      0  \n",
       "1  democrats  0.163  0.692  0.145   -0.1101      0  \n",
       "2  democrats  0.062  0.823  0.115    0.9925      2  \n",
       "3  democrats  0.000  0.903  0.097    0.4215      2  \n",
       "4  democrats  0.261  0.647  0.092   -0.8060      0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test data\n",
    "def split_data(df2020, df2024):\n",
    "    train_size = int(df2020.shape[0] * 0.8)\n",
    "    test_size = min(df2020.shape[0] - train_size, df2024.shape[0])    # choose either 20% or the entirety of the df2024 data (whichever is smaller)\n",
    "\n",
    "    train_df = df2020.sample(n=train_size, random_state=42)      # get random rows\n",
    "    test_df = df2024.sample(n=test_size, random_state=42)\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_dataset, test_dataset = split_data(df2020, df2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 158/158 [00:00<00:00, 411.98 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00<00:00, 379.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Eh, I think that's shortsighted. If someone kills trump, he becomes martyred and all of these protests will lose their power. The American people by and large are usually swayed towards the side of the victim. Right now, the victims are victims of police brutality. If we make the cops or president a bigger victim, public opinion will sway towards them.\\r\\n\\r\\nWe're all just bleeding hearts, at the end of the day. But our hearts bleed for different causes, and different victims. That's why non-violent protests are the number 1 way to go right now, along with writing letters to our representatives. I'm not saying non-violent acts are effective 100% of the time when it comes to producing change, but they do end up being on the morally just side of history more often than the violent acts. They killed Lincoln, and he freed the slaves, and now people side with Lincoln. They killed MLK, and he promoted civil rights, and now people side with MLK. The killers just don't end up as the good guys, nor do they cause positive changes. Instead, violence begets violence, and the blame shifts to whoever is killing. \\r\\n\\r\\nAs soon as people start shooting police, the police start shooting back, and it could mean civil war 2, or something closer to The Troubles in Ireland. It creates the concept of warfare, instead of the concept of peaceful people being violently attacked. It's important to have the right narrative in these situations, and the narrative has to be authentic, honest, and show victims in need of help\", 'created_utc': 1591194766, 'ups': 1, 'subreddit': 'democrats', 'neg': 0.245, 'neu': 0.624, 'pos': 0.131, 'compound': -0.9918, 'label': 0, '__index_level_0__': 65, 'input_ids': [101, 58375, 117, 151, 21506, 10203, 112, 161, 12972, 57246, 22229, 10163, 119, 11526, 25839, 50949, 29104, 117, 10191, 23259, 81035, 10390, 10110, 10367, 10108, 11269, 53697, 11229, 32086, 10487, 11681, 119, 10103, 10600, 11227, 10151, 10110, 12166, 10320, 15976, 17158, 48634, 10163, 17334, 10103, 12029, 10108, 10103, 49848, 119, 12873, 11628, 117, 10103, 33063, 10320, 33063, 10108, 13202, 63573, 12705, 119, 11526, 11312, 12696, 10103, 63648, 10362, 11250, 143, 70579, 49848, 117, 11347, 22556, 11229, 17158, 14519, 17334, 11359, 119, 11312, 112, 11449, 10367, 12125, 10803, 48584, 29642, 117, 10160, 10103, 11421, 10108, 10103, 11111, 119, 10502, 14008, 29642, 10803, 10390, 10139, 12850, 23791, 117, 10110, 12850, 33063, 119, 10203, 112, 161, 18469, 10466, 118, 36397, 53697, 10320, 10103, 11395, 122, 12140, 10114, 11335, 12873, 11628, 117, 12396, 10171, 16155, 18507, 10114, 14008, 25223, 119, 151, 112, 155, 10497, 22811, 10466, 118, 36397, 23923, 10320, 24300, 10445, 110, 10108, 10103, 10573, 10704, 10197, 18408, 10114, 31094, 13780, 117, 10502, 10578, 10154, 11421, 10700, 11352, 10125, 10103, 20604, 10563, 12125, 12029, 10108, 10441, 10772, 13094, 10948, 10103, 36397, 23923, 119, 10578, 15847, 16981, 117, 10110, 10191, 85835, 10103, 44838, 117, 10110, 11628, 11227, 12029, 10171, 16981, 119, 10578, 15847, 32992, 10167, 117, 10110, 10191, 21415, 12346, 14423, 117, 10110, 11628, 11227, 12029, 10171, 32992, 10167, 119, 10103, 80324, 12125, 11530, 112, 162, 11421, 10700, 10146, 10103, 12050, 67922, 117, 24585, 10154, 10578, 15126, 19243, 17992, 119, 16209, 117, 22366, 61601, 17873, 22366, 117, 10110, 10103, 88633, 36731, 10107, 10114, 10488, 44629, 10127, 25045, 119, 10146, 16211, 10146, 11227, 13982, 29268, 13202, 117, 10103, 13202, 13982, 29268, 11677, 117, 10110, 10197, 12296, 25659, 12346, 10313, 123, 117, 10362, 19501, 37260, 10114, 10103, 57477, 10104, 14939, 119, 10197, 66674, 10103, 18174, 10108, 37923, 117, 16209, 10108, 10103, 18174, 10108, 86413, 11227, 11352, 36397, 10563, 30080, 119, 10197, 112, 161, 12652, 10114, 10574, 10103, 12873, 41866, 10104, 11269, 49586, 117, 10110, 10103, 41866, 10438, 10114, 10346, 39253, 88635, 10261, 117, 11817, 14324, 117, 10110, 11391, 33063, 10104, 15415, 10108, 14743, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier output size: 3\n",
      "[0, 0, 2, 0, 0, 1, 2, 0, 0, 2, 2, 2, 1, 0, 2, 1, 2, 0, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 2, 2, 0, 2, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 2, 0, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 1, 0, 2, 2, 0, 2, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 2, 2, 2, 0, 0, 1, 0, 2, 0, 0, 1, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Classifier output size: {model.classifier.out_features}\")\n",
    "print(train_dataset['label'][:100]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "# test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\surfd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/50 [29:51<?, ?it/s]\n",
      " 20%|â–ˆâ–ˆ        | 10/50 [03:08<12:04, 18.11s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                               \n",
      " 20%|â–ˆâ–ˆ        | 10/50 [03:26<12:04, 18.11s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8660032153129578, 'eval_accuracy': 0.55, 'eval_runtime': 17.4333, 'eval_samples_per_second': 2.294, 'eval_steps_per_second': 0.172, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [07:16<14:10, 28.34s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                               \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [08:08<14:10, 28.34s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8487539291381836, 'eval_accuracy': 0.6, 'eval_runtime': 52.3692, 'eval_samples_per_second': 0.764, 'eval_steps_per_second': 0.057, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [11:49<08:00, 24.01s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                               \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [12:06<08:00, 24.01s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9155132174491882, 'eval_accuracy': 0.525, 'eval_runtime': 17.4396, 'eval_samples_per_second': 2.294, 'eval_steps_per_second': 0.172, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [15:57<04:37, 27.78s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                               \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [16:15<04:37, 27.78s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9621081352233887, 'eval_accuracy': 0.55, 'eval_runtime': 17.6977, 'eval_samples_per_second': 2.26, 'eval_steps_per_second': 0.17, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [22:10<00:00, 26.76s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                               \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [22:54<00:00, 26.76s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9641491770744324, 'eval_accuracy': 0.525, 'eval_runtime': 39.5872, 'eval_samples_per_second': 1.01, 'eval_steps_per_second': 0.076, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [22:58<00:00, 27.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1378.861, 'train_samples_per_second': 0.573, 'train_steps_per_second': 0.036, 'train_loss': 0.5398244476318359, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Log: {'eval_loss': 0.8487539291381836, 'eval_accuracy': 0.6, 'eval_runtime': 17.4915, 'eval_samples_per_second': 2.287, 'eval_steps_per_second': 0.172, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    logging_dir='./logs'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "eval_log = None\n",
    "trainer.train()\n",
    "eval_log = trainer.evaluate()\n",
    "\n",
    "\n",
    "print(f'Evaluation Log: {eval_log}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 38\u001b[0m\n\u001b[0;32m     34\u001b[0m     plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Label\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     36\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 38\u001b[0m \u001b[43mfinetune_confusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFinetuned Model Confusion Matrix for Comments\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[80], line 11\u001b[0m, in \u001b[0;36mfinetune_confusion\u001b[1;34m(test_dataset, fig_name)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Iterate over the test dataset\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m test_dataloader:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Get inputs and labels, move them to the device\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[0;32m     12\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     13\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "def finetune_confusion(test_dataset, fig_name):\n",
    "    batch_size = 32  # Set the batch size\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    # Iterate over the test dataset\n",
    "    for batch in test_dataloader:\n",
    "        # Get inputs and labels, move them to the device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Get predicted class (argmax on logits)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        # Append to lists\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(cm)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "\n",
    "    plt.title(fig_name)\n",
    "    plt.ylabel('Ground Truth Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "finetune_confusion(test_dataset, 'Finetuned Model Confusion Matrix for Comments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2020 = relabel('./data/labeled_posts_2020.csv')\n",
    "pdf2024 = relabel('./data/labeled_posts_2024.csv')\n",
    "\n",
    "train_dataset, test_dataset = split_data(df2020, df2024)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    logging_dir='./logs'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "eval_log = None\n",
    "trainer.train()\n",
    "eval_log = trainer.evaluate()\n",
    "\n",
    "\n",
    "print(f'Evaluation Log: {eval_log}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
